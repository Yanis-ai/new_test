下面给出两个部分的内容：

---

### 1. untar.py 脚本

这个脚本会从 S3 下载传入的 tar.gz 文件，解压后将所有解压出的文件上传到 S3 桶中 "output" 文件夹下。请确保你的容器镜像中包含此脚本，并且镜像安装了 boto3 模块。

```python
#!/usr/bin/env python3

import argparse
import json
import os
import tarfile
import boto3
import tempfile

def main():
    parser = argparse.ArgumentParser(
        description='Download tar.gz files from S3, extract them, and upload extracted contents to the output folder in S3.'
    )
    parser.add_argument('--bucket', required=True, help='S3 bucket name')
    parser.add_argument('--files', required=True, help='JSON string list of tar.gz file keys')
    args = parser.parse_args()
    
    bucket = args.bucket
    try:
        file_list = json.loads(args.files)
    except Exception as e:
        print("Error parsing files argument:", e)
        return
    
    s3 = boto3.client('s3')
    
    # 创建临时目录用于下载和解压
    with tempfile.TemporaryDirectory() as tmpdir:
        for s3_key in file_list:
            local_tar_path = os.path.join(tmpdir, os.path.basename(s3_key))
            print(f"Downloading {s3_key} to {local_tar_path}...")
            s3.download_file(bucket, s3_key, local_tar_path)
            
            # 解压 tar.gz 文件
            try:
                with tarfile.open(local_tar_path, "r:gz") as tar:
                    tar.extractall(path=tmpdir)
                print(f"Extracted {local_tar_path} successfully.")
            except Exception as e:
                print(f"Failed to extract {local_tar_path}: {e}")
                continue
            
            # 上传所有解压出的文件（排除原 tar 文件），目标路径前缀为 output/
            for root, dirs, files in os.walk(tmpdir):
                for file in files:
                    file_path = os.path.join(root, file)
                    # 排除刚下载的 tar 文件
                    if file_path == local_tar_path:
                        continue
                    # 计算相对于临时目录的相对路径
                    rel_path = os.path.relpath(file_path, tmpdir)
                    dest_key = os.path.join("output", rel_path).replace("\\", "/")
                    print(f"Uploading {file_path} to s3://{bucket}/{dest_key}...")
                    s3.upload_file(file_path, bucket, dest_key)
                    
    print("Processing completed.")

if __name__ == "__main__":
    main()
```

---

### 2. 修正后的 Terraform 代码

以下 Terraform 代码示例中，主要修改在 AWS Batch 作业定义部分：
- 增加了 `executionRoleArn` 字段（使用你为 Fargate 模式创建的 ECS 任务执行角色）。
- 确保 Batch 作业中传入的参数与 untar.py 脚本一致（传入 S3 桶和 tar.gz 文件列表）。

请注意下面代码中其它资源（如 VPC 模块、S3 桶、ECR、Lambda、IAM 角色等）与之前版本类似，只是在 Batch 部分做了修正。

```hcl
provider "aws" {
  region = "ap-northeast-1"
}

variable "lambda_runtime" {
  description = "Lambda 运行时环境"
  type        = string
  default     = "python3.9"
}

variable "batch_job_queue_name" {
  description = "AWS Batch 作业队列名称"
  type        = string
  default     = "untar-job-queue"
}

variable "batch_job_definition_name" {
  description = "AWS Batch 作业定义名称"
  type        = string
  default     = "untar-job-definition"
}

variable "test_prefix" {
  description = "バッチテスト環境のプレフィックス"
  type        = string
  default     = "batch-test"
}

module "vpc" {
  source      = "./modules/vpc"
  test_prefix = var.test_prefix
}

resource "random_string" "suffix" {
  length  = 8
  special = false
  upper   = false
}

#################################
# S3 桶：存放输入文件（tar.gz） #
#################################
resource "aws_s3_bucket" "s3input" {
  bucket        = "batch-bucket-${random_string.suffix.result}"
  force_destroy = true
}

#################################
# ECR 仓库：存放解压容器镜像  #
#################################
resource "aws_ecr_repository" "untar_repo" {
  depends_on   = [ aws_s3_bucket.s3input ]
  name         = "untar-repo"
  force_delete = true
}

#################################
# Lambda 函数及相关 IAM         #
#################################
data "archive_file" "lambda_zip" {
  type        = "zip"
  source_file = "${path.module}/lambda_function.py"
  output_path = "${path.module}/lambda_function.zip"
}

resource "aws_iam_role" "lambda_exec_role" {
  depends_on = [ aws_ecr_repository.untar_repo ]
  name       = "lambda_exec_role"
  assume_role_policy = jsonencode({
    Version   = "2012-10-17",
    Statement = [{
      Action    = "sts:AssumeRole",
      Effect    = "Allow",
      Principal = { Service = "lambda.amazonaws.com" }
    }]
  })
}

resource "aws_iam_role_policy_attachment" "lambda_basic" {
  depends_on = [ aws_iam_role.lambda_exec_role ]
  role       = aws_iam_role.lambda_exec_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
}

resource "aws_iam_policy" "lambda_custom_policy" {
  depends_on = [ aws_iam_role_policy_attachment.lambda_basic ]
  name       = "lambda_custom_policy"
  policy     = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Action = [
          "s3:ListBucket",
          "s3:GetObject"
        ],
        Resource = [
          aws_s3_bucket.s3input.arn,
          "${aws_s3_bucket.s3input.arn}/*"
        ]
      },
      {
        Effect   = "Allow",
        Action   = [ "batch:SubmitJob" ],
        Resource = "*"
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "lambda_custom_policy_attachment" {
  role       = aws_iam_role.lambda_exec_role.name
  policy_arn = aws_iam_policy.lambda_custom_policy.arn
}

resource "null_resource" "execute_and_upload" {
  depends_on = [aws_s3_bucket.s3input]

  provisioner "local-exec" {
    command = <<EOT
      ./00_generate_and_upload_script.sh
      aws s3 cp ./testfiles/ s3://${aws_s3_bucket.s3input.bucket}/input/ --recursive
      rm -f ./testfiles/*
      ./01_push_docker_to_ecr.sh
    EOT
  }
}

resource "aws_lambda_function" "untar_lambda" {
  function_name = "untar_lambda"
  role          = aws_iam_role.lambda_exec_role.arn
  handler       = "lambda_function.lambda_handler"
  runtime       = var.lambda_runtime
  filename      = data.archive_file.lambda_zip.output_path
  timeout       = 900
  memory_size   = 512

  environment {
    variables = {
      S3_BUCKET            = aws_s3_bucket.s3input.id
      BATCH_JOB_DEFINITION = var.batch_job_definition_name
      BATCH_JOB_QUEUE      = var.batch_job_queue_name
    }
  }
}

resource "aws_lambda_permission" "s3_lambda" {
  statement_id  = "AllowS3InvokeLambda"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.untar_lambda.function_name
  principal     = "s3.amazonaws.com"
  source_arn    = aws_s3_bucket.s3input.arn
}

#################################
# AWS Batch 资源                #
#################################
resource "aws_iam_role" "batch_service_role" {
  name = "batch_service_role"
  assume_role_policy = jsonencode({
    Version   = "2012-10-17",
    Statement = [{
      Effect    = "Allow",
      Principal = { Service = "batch.amazonaws.com" },
      Action    = "sts:AssumeRole"
    }]
  })
}

resource "aws_iam_role_policy_attachment" "batch_service_policy_attachment" {
  role       = aws_iam_role.batch_service_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole"
}

resource "aws_iam_role" "ecs_execution_role" {
  name = "ecs-exection-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Principal = { Service = "ecs-tasks.amazonaws.com" },
      Action = "sts:AssumeRole"
    }]
  })
}

resource "aws_iam_role_policy_attachment" "ecs_execution_policy" {
  role       = aws_iam_role.ecs_execution_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy"
}

resource "aws_security_group" "batch_sg" {
  name        = "batch_sg"
  description = "Security group for AWS Batch compute environment"
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port   = 0
    to_port     = 65535
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_batch_compute_environment" "batch_compute_env" {
  compute_environment_name = "batch_compute_env"
  service_role             = aws_iam_role.batch_service_role.arn
  type                     = "MANAGED"

  compute_resources {
    type               = "FARGATE"
    max_vcpus          = 2
    subnets            = module.vpc.subnet_ids
    security_group_ids = [aws_security_group.batch_sg.id]
  }
}

resource "aws_batch_job_queue" "batch_job_queue" {
  name     = var.batch_job_queue_name
  state    = "ENABLED"
  priority = 1

  compute_environment_order {
    order               = 1
    compute_environment = aws_batch_compute_environment.batch_compute_env.arn
  }
}

resource "aws_batch_job_definition" "untar_job_definition" {
  name = var.batch_job_definition_name
  type = "container"

  container_properties = jsonencode({
    image            = "${aws_ecr_repository.untar_repo.repository_url}:latest",
    command          = [
      "python",
      "untar.py",
      "--bucket",
      "Ref::bucket",
      "--files",
      "Ref::files"
    ],
    executionRoleArn = aws_iam_role.ecs_execution_role.arn,
    resourceRequirements = [
      {
        type  = "VCPU",
        value = "1"
      },
      {
        type  = "MEMORY",
        value = "2048"
      }
    ]
  })
   
  parameters = {
    bucket = "",
    files  = ""
  }
  platform_capabilities = [ "FARGATE" ]
}
```

---

### 说明

- **untar.py**  
  脚本解析命令行参数，从 S3 下载 tar.gz 文件、解压后将所有提取的文件上传到目标 S3 桶下的 `output/` 文件夹。

- **Batch 作业定义**  
  更新后的 `aws_batch_job_definition` 增加了 `executionRoleArn` 字段，指向 ECS 任务执行角色，满足 Fargate 作业要求。

将上述文件保存后，请确保：
1. `untar.py` 被包含在构建 ECR 镜像的 Dockerfile 中；
2. Terraform 中的其它资源（如 VPC 模块、Lambda、S3 桶）按需调整；
3. 使用 `terraform init` 和 `terraform apply` 部署更新后的基础设施。

这样，当 Lambda 触发 AWS Batch 作业时，Batch 作业会调用容器内的 `untar.py`，下载输入文件、解压并将解压后的文件上传到 S3 的 `output/` 文件夹中。

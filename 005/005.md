下面提供了修改后的 Terraform 脚本示例，将 AWS Batch 的计算环境改为基于 Fargate 模式，从而无需管理 EC2 实例。脚本包含以下资源：

- S3 桶及事件通知（新上传的 tar.gz 文件自动触发 Lambda）  
- ECR 仓库（存放解压作业的容器镜像）  
- Lambda 函数（扫描 S3、分组文件并提交 Batch 作业）  
- AWS Batch 所需的 IAM 角色、Fargate 计算环境、作业队列和作业定义

请根据实际情况（例如 VPC、子网、镜像内解压脚本 untar.py 等）调整相关配置。

```hcl
#############################
# Provider 及变量定义      #
#############################

provider "aws" {
  region = "us-east-1"    # 根据实际情况修改区域
}

variable "s3_bucket_name" {
  description = "存放 tar.gz 文件的 S3 桶名称"
  type        = string
}

variable "s3_prefix" {
  description = "S3 桶中 tar.gz 文件的前缀（如果有）"
  type        = string
  default     = ""
}

variable "batch_job_queue_name" {
  description = "AWS Batch 作业队列名称"
  type        = string
  default     = "untar-job-queue"
}

variable "batch_job_definition_name" {
  description = "AWS Batch 作业定义名称"
  type        = string
  default     = "untar-job-definition"
}

variable "vpc_id" {
  description = "Batch 计算环境所使用的 VPC ID"
  type        = string
}

variable "subnet_ids" {
  description = "Batch 计算环境使用的子网 ID 列表"
  type        = list(string)
}

variable "lambda_runtime" {
  description = "Lambda 运行时环境"
  type        = string
  default     = "python3.8"
}

#############################
# S3 桶及事件通知           #
#############################

resource "aws_s3_bucket" "tar_bucket" {
  bucket = var.s3_bucket_name
  acl    = "private"
}

resource "aws_s3_bucket_notification" "tar_notification" {
  bucket = aws_s3_bucket.tar_bucket.id

  lambda_function {
    lambda_function_arn = aws_lambda_function.untar_lambda.arn
    events              = ["s3:ObjectCreated:*"]
    filter_prefix       = var.s3_prefix
    filter_suffix       = ".tar.gz"
  }
}

#############################
# ECR 仓库                  #
#############################

resource "aws_ecr_repository" "untar_repo" {
  name = "untar-repo"
}

#############################
# Lambda 函数及相关 IAM    #
#############################

# 打包 Lambda 代码（假设 lambda_function.py 在当前目录）
data "archive_file" "lambda_zip" {
  type        = "zip"
  source_file = "${path.module}/lambda_function.py"
  output_path = "${path.module}/lambda_function.zip"
}

# Lambda 执行角色
resource "aws_iam_role" "lambda_exec_role" {
  name = "lambda_exec_role"
  assume_role_policy = jsonencode({
    Version   = "2012-10-17",
    Statement = [{
      Action    = "sts:AssumeRole",
      Effect    = "Allow",
      Principal = { Service = "lambda.amazonaws.com" }
    }]
  })
}

# 附加 CloudWatch Logs 权限
resource "aws_iam_role_policy_attachment" "lambda_basic" {
  role       = aws_iam_role.lambda_exec_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
}

# 自定义策略：允许列举 S3 桶、读取对象以及提交 Batch 作业
resource "aws_iam_policy" "lambda_custom_policy" {
  name   = "lambda_custom_policy"
  policy = jsonencode({
    Version   = "2012-10-17",
    Statement = [
      {
        Effect   = "Allow",
        Action   = [
          "s3:ListBucket",
          "s3:GetObject"
        ],
        Resource = [
          aws_s3_bucket.tar_bucket.arn,
          "${aws_s3_bucket.tar_bucket.arn}/*"
        ]
      },
      {
        Effect   = "Allow",
        Action   = [
          "batch:SubmitJob"
        ],
        Resource = "*"
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "lambda_custom_policy_attachment" {
  role       = aws_iam_role.lambda_exec_role.name
  policy_arn = aws_iam_policy.lambda_custom_policy.arn
}

# 创建 Lambda 函数
resource "aws_lambda_function" "untar_lambda" {
  function_name = "untar_lambda"
  role          = aws_iam_role.lambda_exec_role.arn
  handler       = "lambda_function.lambda_handler"
  runtime       = var.lambda_runtime
  filename      = data.archive_file.lambda_zip.output_path

  environment {
    variables = {
      S3_BUCKET            = aws_s3_bucket.tar_bucket.id
      S3_PREFIX            = var.s3_prefix
      BATCH_JOB_DEFINITION = var.batch_job_definition_name
      BATCH_JOB_QUEUE      = var.batch_job_queue_name
    }
  }
}

# 允许 S3 调用 Lambda
resource "aws_lambda_permission" "s3_lambda" {
  statement_id  = "AllowS3InvokeLambda"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.untar_lambda.function_name
  principal     = "s3.amazonaws.com"
  source_arn    = aws_s3_bucket.tar_bucket.arn
}

#############################
# AWS Batch 资源           #
#############################

# Batch 服务角色（供 Batch 调度使用）
resource "aws_iam_role" "batch_service_role" {
  name = "batch_service_role"
  assume_role_policy = jsonencode({
    Version   = "2012-10-17",
    Statement = [{
      Effect    = "Allow",
      Principal = { Service = "batch.amazonaws.com" },
      Action    = "sts:AssumeRole"
    }]
  })
}

resource "aws_iam_role_policy_attachment" "batch_service_policy_attachment" {
  role       = aws_iam_role.batch_service_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole"
}

# 安全组，用于 Batch Fargate 计算环境
resource "aws_security_group" "batch_sg" {
  name        = "batch_sg"
  description = "Security group for AWS Batch compute environment"
  vpc_id      = var.vpc_id

  ingress {
    from_port   = 0
    to_port     = 65535
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# AWS Batch 计算环境（Fargate 模式）
resource "aws_batch_compute_environment" "batch_compute_env" {
  compute_environment_name = "batch_compute_env"
  service_role             = aws_iam_role.batch_service_role.arn
  type                     = "MANAGED"

  compute_resources {
    type               = "FARGATE"
    max_vcpus          = 4
    subnets            = var.subnet_ids
    security_group_ids = [aws_security_group.batch_sg.id]
  }
}

# AWS Batch 作业队列
resource "aws_batch_job_queue" "batch_job_queue" {
  name     = var.batch_job_queue_name
  state    = "ENABLED"
  priority = 1

  compute_environments = [aws_batch_compute_environment.batch_compute_env.arn]
}

# AWS Batch 作业定义
# 注意：Fargate 模式下，vcpus 与内存需满足 Fargate 的规格要求（例如 1 vcpu 对应 2048 MB 内存）
resource "aws_batch_job_definition" "untar_job_definition" {
  name = var.batch_job_definition_name
  type = "container"

  container_properties = jsonencode({
    image   = "${aws_ecr_repository.untar_repo.repository_url}:latest",
    vcpus   = 1,
    memory  = 2048,
    command = [
      "python",
      "untar.py",
      "--bucket",
      "Ref::bucket",
      "--files",
      "Ref::files"
    ]
  })

  parameters = {
    bucket = "",
    files  = ""
  }
}

#############################
# 输出信息                 #
#############################

output "lambda_function_arn" {
  description = "Lambda 函数 ARN"
  value       = aws_lambda_function.untar_lambda.arn
}

output "ecr_repository_url" {
  description = "ECR 仓库 URL"
  value       = aws_ecr_repository.untar_repo.repository_url
}

output "batch_compute_environment" {
  description = "Batch 计算环境 ARN"
  value       = aws_batch_compute_environment.batch_compute_env.arn
}

output "batch_job_queue" {
  description = "Batch 作业队列 ARN"
  value       = aws_batch_job_queue.batch_job_queue.arn
}

output "batch_job_definition" {
  description = "Batch 作业定义 ARN"
  value       = aws_batch_job_definition.untar_job_definition.arn
}
```

---

### 说明

1. **S3 与 Lambda**  
   - 创建 S3 桶用于存放 tar.gz 文件，并配置事件通知，当符合前缀和后缀的对象上传时触发 Lambda。  
   - Lambda 函数读取 S3 文件列表、按每组最多两个文件进行分组，并调用 AWS Batch 提交作业。

2. **ECR**  
   - 用于存放包含解压逻辑（untar.py）的容器镜像，请确保镜像已上传至该仓库，并打上合适的标签（此处示例中使用 latest 标签）。

3. **AWS Batch（Fargate 计算环境）**  
   - 使用 Fargate 模式时，在 compute_resources 中设置 type = "FARGATE"，无需 EC2 实例角色。  
   - 请确保 Batch 作业定义中 vcpus 与 memory 的组合符合 Fargate 规格要求（例如 1 vcpu 对应至少 2048 MB 内存）。

4. **IAM 权限**  
   - Lambda 与 Batch 均配置了必要的 IAM 角色和策略，确保具有访问 S3、ECR、以及提交 Batch 作业的权限。

将以上 Terraform 代码保存到一个文件（例如 main.tf），并确保 lambda_function.py 文件存在于同目录。然后依次执行：

```bash
terraform init
terraform apply
```

这样整个流程就会创建完成，并且 AWS Batch 计算环境将使用 Fargate 模式来运行你的解压作业。

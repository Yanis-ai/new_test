下面提供一个 Python 示例的 Lambda 函数代码，用于实现以下逻辑：

- 列举指定 S3 桶（和可选前缀）下的 tar.gz 文件  
- 将文件列表按照每组最多两个文件进行分组  
- 针对每一组文件调用 AWS Batch 的作业提交接口，将 S3 桶名称及文件列表作为参数传入

请确保在 Lambda 的环境变量中配置以下信息：
- **S3_BUCKET**：待处理文件所在的 S3 桶名称  
- **S3_PREFIX**（可选）：S3 桶中文件的前缀  
- **BATCH_JOB_DEFINITION**：AWS Batch 作业定义名称  
- **BATCH_JOB_QUEUE**：AWS Batch 作业队列名称

下面是示例代码：

```python
import json
import boto3
import os

def lambda_handler(event, context):
    # 从环境变量中读取必要的配置信息
    bucket_name = os.environ.get('S3_BUCKET')
    s3_prefix = os.environ.get('S3_PREFIX', '')
    job_definition = os.environ.get('BATCH_JOB_DEFINITION')
    job_queue = os.environ.get('BATCH_JOB_QUEUE')
    
    # 初始化 boto3 客户端
    s3_client = boto3.client('s3')
    batch_client = boto3.client('batch')
    
    # 列出 S3 桶中指定前缀下的对象
    try:
        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=s3_prefix)
    except Exception as e:
        print(f"列举 S3 对象时出错：{e}")
        return {
            'statusCode': 500,
            'body': json.dumps('列举 S3 对象失败')
        }
    
    if 'Contents' not in response:
        print("没有找到任何对象")
        return {
            'statusCode': 200,
            'body': json.dumps('S3 桶中没有对象')
        }
    
    # 筛选出所有以 .tar.gz 结尾的文件
    tar_files = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.tar.gz')]
    
    if not tar_files:
        print("没有找到 tar.gz 文件")
        return {
            'statusCode': 200,
            'body': json.dumps('没有找到 tar.gz 文件')
        }
    
    print(f"找到 {len(tar_files)} 个 tar.gz 文件")
    
    # 将文件列表按每组最多两个文件分组
    groups = [tar_files[i:i + 2] for i in range(0, len(tar_files), 2)]
    
    submitted_jobs = []
    # 针对每一组文件提交一个 AWS Batch 作业
    for index, group in enumerate(groups, start=1):
        job_name = f"untar-job-{index}"
        
        # 通过作业参数传递 S3 桶名和当前组的文件列表
        # 注意：在 Batch 任务中，容器启动后需要解析该参数，并依次下载解压文件
        parameters = {
            'bucket': bucket_name,
            'files': json.dumps(group)  # 将列表转换为 JSON 字符串
        }
        
        try:
            response = batch_client.submit_job(
                jobName=job_name,
                jobQueue=job_queue,
                jobDefinition=job_definition,
                parameters=parameters
            )
            job_id = response['jobId']
            submitted_jobs.append(job_id)
            print(f"提交作业 {job_name} 成功，jobId: {job_id}，处理文件: {group}")
        except Exception as e:
            print(f"提交作业 {job_name} 时出错：{e}")
    
    return {
        'statusCode': 200,
        'body': json.dumps({'submitted_jobs': submitted_jobs})
    }
```

### 说明

- **S3 文件列表读取**：通过 `list_objects_v2` 接口获取 S3 桶中指定前缀下的对象，并筛选出扩展名为 `.tar.gz` 的文件。  
- **分组逻辑**：采用列表切片的方式，每组最多两个文件，生成一个二维列表。  
- **AWS Batch 作业提交**：使用 boto3 的 Batch 客户端逐组提交作业，作业参数中包含 S3 桶名称以及当前组文件列表（转换为 JSON 字符串传递）。  
- **容器内处理**：在 Batch 作业运行的容器中，需要解析传入的参数，从 S3 下载对应文件并完成解压任务。  

请根据实际业务需要和环境配置（如 IAM 权限、网络配置等）进行调整。

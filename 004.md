下面给出一个完整的 Terraform 示例，包含整个流程所需的所有资源：  
- S3 桶（存放 tar.gz 文件，并配置事件通知触发 Lambda）  
- ECR 仓库（存放解压逻辑的容器镜像）  
- Lambda 函数（扫描 S3 文件、分组并调用 AWS Batch 提交作业）  
- AWS Batch 所需的 IAM 角色、计算环境、作业队列和作业定义

> 注意：  
> - 你需要准备好 Lambda 函数代码文件（例如放在同目录下的 *lambda_function.py*），以及容器中运行的解压脚本（镜像中需包含 untar.py 脚本，并打包上传到 ECR）。  
> - 部分资源（如 VPC、子网）需根据你实际环境配置。  

下面的示例采用单个 Terraform 文件（比如 main.tf），你可以根据需要拆分文件。

```hcl
#############################
# Provider 及变量定义      #
#############################

provider "aws" {
  region = "us-east-1"    # 根据实际情况修改区域
}

variable "s3_bucket_name" {
  description = "存放 tar.gz 文件的 S3 桶名称"
  type        = string
}

variable "s3_prefix" {
  description = "S3 桶中 tar.gz 文件的前缀（如果有）"
  type        = string
  default     = ""
}

variable "batch_job_queue_name" {
  description = "AWS Batch 作业队列名称"
  type        = string
  default     = "untar-job-queue"
}

variable "batch_job_definition_name" {
  description = "AWS Batch 作业定义名称"
  type        = string
  default     = "untar-job-definition"
}

variable "vpc_id" {
  description = "Batch 计算环境所使用的 VPC ID"
  type        = string
}

variable "subnet_ids" {
  description = "Batch 计算环境使用的子网 ID 列表"
  type        = list(string)
}

variable "lambda_runtime" {
  description = "Lambda 运行时环境"
  type        = string
  default     = "python3.8"
}

#############################
# S3 桶及事件通知           #
#############################

resource "aws_s3_bucket" "tar_bucket" {
  bucket = var.s3_bucket_name
  acl    = "private"
}

# 配置 S3 事件通知，当新建 tar.gz 对象时触发 Lambda（注意 S3 通知需要在 Lambda 权限允许下生效）
resource "aws_s3_bucket_notification" "tar_notification" {
  bucket = aws_s3_bucket.tar_bucket.id

  lambda_function {
    lambda_function_arn = aws_lambda_function.untar_lambda.arn
    events              = ["s3:ObjectCreated:*"]
    filter_prefix       = var.s3_prefix
    filter_suffix       = ".tar.gz"
  }
}

#############################
# ECR 仓库                  #
#############################

resource "aws_ecr_repository" "untar_repo" {
  name = "untar-repo"
}

#############################
# Lambda 函数及相关 IAM    #
#############################

# 使用 archive_file 将 lambda_function.py 打包为 zip 文件
data "archive_file" "lambda_zip" {
  type        = "zip"
  source_file = "${path.module}/lambda_function.py"
  output_path = "${path.module}/lambda_function.zip"
}

# Lambda 执行角色
resource "aws_iam_role" "lambda_exec_role" {
  name = "lambda_exec_role"
  assume_role_policy = jsonencode({
    Version   = "2012-10-17",
    Statement = [{
      Action    = "sts:AssumeRole",
      Effect    = "Allow",
      Principal = { Service = "lambda.amazonaws.com" }
    }]
  })
}

# 附加 CloudWatch Logs 权限
resource "aws_iam_role_policy_attachment" "lambda_basic" {
  role       = aws_iam_role.lambda_exec_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
}

# 自定义策略：允许列举 S3 桶、读取对象以及提交 Batch 作业
resource "aws_iam_policy" "lambda_custom_policy" {
  name   = "lambda_custom_policy"
  policy = jsonencode({
    Version   = "2012-10-17",
    Statement = [
      {
        Effect   = "Allow",
        Action   = [
          "s3:ListBucket",
          "s3:GetObject"
        ],
        Resource = [
          aws_s3_bucket.tar_bucket.arn,
          "${aws_s3_bucket.tar_bucket.arn}/*"
        ]
      },
      {
        Effect   = "Allow",
        Action   = [
          "batch:SubmitJob"
        ],
        Resource = "*"
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "lambda_custom_policy_attachment" {
  role       = aws_iam_role.lambda_exec_role.name
  policy_arn = aws_iam_policy.lambda_custom_policy.arn
}

# 创建 Lambda 函数
resource "aws_lambda_function" "untar_lambda" {
  function_name = "untar_lambda"
  role          = aws_iam_role.lambda_exec_role.arn
  handler       = "lambda_function.lambda_handler"
  runtime       = var.lambda_runtime
  filename      = data.archive_file.lambda_zip.output_path

  environment {
    variables = {
      S3_BUCKET            = aws_s3_bucket.tar_bucket.id
      S3_PREFIX            = var.s3_prefix
      BATCH_JOB_DEFINITION = var.batch_job_definition_name
      BATCH_JOB_QUEUE      = var.batch_job_queue_name
    }
  }
}

# 允许 S3 调用 Lambda
resource "aws_lambda_permission" "s3_lambda" {
  statement_id  = "AllowS3InvokeLambda"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.untar_lambda.function_name
  principal     = "s3.amazonaws.com"
  source_arn    = aws_s3_bucket.tar_bucket.arn
}

#############################
# AWS Batch 资源           #
#############################

### Batch 服务角色（供 Batch 调度使用）
resource "aws_iam_role" "batch_service_role" {
  name = "batch_service_role"
  assume_role_policy = jsonencode({
    Version   = "2012-10-17",
    Statement = [{
      Effect    = "Allow",
      Principal = { Service = "batch.amazonaws.com" },
      Action    = "sts:AssumeRole"
    }]
  })
}

resource "aws_iam_role_policy_attachment" "batch_service_policy_attachment" {
  role       = aws_iam_role.batch_service_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole"
}

### Batch 计算环境所使用的 EC2 实例角色
resource "aws_iam_role" "batch_instance_role" {
  name = "batch_instance_role"
  assume_role_policy = jsonencode({
    Version   = "2012-10-17",
    Statement = [{
      Effect    = "Allow",
      Principal = { Service = "ec2.amazonaws.com" },
      Action    = "sts:AssumeRole"
    }]
  })
}

resource "aws_iam_role_policy_attachment" "batch_instance_policy_attachment" {
  role       = aws_iam_role.batch_instance_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role"
}

# Batch 实例配置文件
resource "aws_iam_instance_profile" "batch_instance_profile" {
  name = "batch_instance_profile"
  role = aws_iam_role.batch_instance_role.name
}

# 为 Batch 计算环境创建安全组（使用指定 VPC）
resource "aws_security_group" "batch_sg" {
  name        = "batch_sg"
  description = "Security group for AWS Batch compute environment"
  vpc_id      = var.vpc_id

  ingress {
    from_port   = 0
    to_port     = 65535
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# Batch 计算环境（EC2 类型）
resource "aws_batch_compute_environment" "batch_compute_env" {
  compute_environment_name = "batch_compute_env"
  service_role             = aws_iam_role.batch_service_role.arn
  type                     = "MANAGED"

  compute_resources {
    instance_role      = aws_iam_instance_profile.batch_instance_profile.arn
    instance_types     = ["m4.large"]
    max_vcpus          = 4
    min_vcpus          = 0
    desired_vcpus      = 0
    security_group_ids = [aws_security_group.batch_sg.id]
    subnets            = var.subnet_ids
    type               = "EC2"
  }
}

# Batch 作业队列
resource "aws_batch_job_queue" "batch_job_queue" {
  name                 = var.batch_job_queue_name
  state                = "ENABLED"
  priority             = 1

  compute_environments = [aws_batch_compute_environment.batch_compute_env.arn]
}

# Batch 作业定义  
# 注意：容器镜像引用的是上面创建的 ECR 仓库，本示例假设镜像中包含 untar.py 脚本，
# 且通过命令行参数（--bucket 与 --files）获取要处理的 S3 桶和文件列表
resource "aws_batch_job_definition" "untar_job_definition" {
  name = var.batch_job_definition_name
  type = "container"

  container_properties = jsonencode({
    image   = "${aws_ecr_repository.untar_repo.repository_url}:latest",
    vcpus   = 1,
    memory  = 512,
    command = [
      "python",
      "untar.py",
      "--bucket",
      "Ref::bucket",
      "--files",
      "Ref::files"
    ]
  })

  parameters = {
    bucket = "",
    files  = ""
  }
}

#############################
# 输出信息                 #
#############################

output "lambda_function_arn" {
  description = "Lambda 函数 ARN"
  value       = aws_lambda_function.untar_lambda.arn
}

output "ecr_repository_url" {
  description = "ECR 仓库 URL"
  value       = aws_ecr_repository.untar_repo.repository_url
}

output "batch_compute_environment" {
  description = "Batch 计算环境 ARN"
  value       = aws_batch_compute_environment.batch_compute_env.arn
}

output "batch_job_queue" {
  description = "Batch 作业队列 ARN"
  value       = aws_batch_job_queue.batch_job_queue.arn
}

output "batch_job_definition" {
  description = "Batch 作业定义 ARN"
  value       = aws_batch_job_definition.untar_job_definition.arn
}
```

---

### 说明

1. **S3 桶与事件通知**  
   创建 S3 桶用于存放 tar.gz 文件，并配置事件通知，当对象创建且符合指定前缀和后缀（.tar.gz）时触发 Lambda 函数。

2. **ECR 仓库**  
   用于存放解压用的容器镜像。镜像需包含解压逻辑脚本（例如 untar.py）。

3. **Lambda 函数**  
   - 通过 archive_file 数据源将本地的 *lambda_function.py* 打包上传。  
   - 环境变量传递 S3 桶、前缀以及 Batch 作业定义和作业队列信息。  
   - Lambda 执行角色同时获得访问 S3 和提交 Batch 作业的权限。  
   - S3 桶通知配置与 Lambda 权限关联，允许 S3 调用 Lambda。

4. **AWS Batch**  
   - 创建 Batch 服务角色、EC2 实例角色及对应的实例配置文件，并指定安全组和子网。  
   - 计算环境、作业队列和作业定义配置完毕，作业定义中通过命令行参数（使用 Batch 参数替换语法）传递 S3 桶和文件列表。

将上述 Terraform 脚本保存后，放置你的 *lambda_function.py* 文件在同目录，然后依次执行：

```bash
terraform init
terraform apply
```

这样整个流程的基础设施即会创建完毕。你需要确保 ECR 中已上传正确的容器镜像，并配置好容器内的解压逻辑。

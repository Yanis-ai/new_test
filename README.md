å¥½çš„ï¼Œä¸‹é¢æˆ‘å°†æä¾›å®Œæ•´çš„ä»£ç ç»“æ„å’Œæ¯ä¸ªæ–‡ä»¶çš„å…·ä½“ä»£ç ï¼ŒåŸºäºæ‚¨çš„éœ€æ±‚ï¼šæ ¹æ® S3 ä¸­å‹ç¼©æ–‡ä»¶çš„æ•°é‡æ¥åŠ¨æ€å†³å®šå¯åŠ¨å¤šå°‘ä¸ª ECS å®¹å™¨é•œåƒè¿›è¡Œè§£å‹å¤„ç†ã€‚

---

## **ğŸ“ ç›®å½•ç»“æ„**

```
ğŸ“‚ terraform-unzip
â”œâ”€â”€ ğŸ“‚ lambda
â”‚   â”œâ”€â”€ ğŸ“œ lambda_function.py           # è§¦å‘ AWS Batch ä»»åŠ¡çš„ Lambda å‡½æ•°
â”‚   â”œâ”€â”€ ğŸ“œ calculate_time_lambda.py     # è®¡ç®—è§£å‹æ€»æ—¶é—´çš„ Lambda å‡½æ•°
â”‚   â”œâ”€â”€ ğŸ“œ requirements.txt             # Lambda ä¾èµ–åŒ…
â”‚   â”œâ”€â”€ ğŸ“œ zip_lambda.sh                # æ‰“åŒ… Lambda ä»£ç è„šæœ¬
â”œâ”€â”€ ğŸ“‚ ecs-unzip
â”‚   â”œâ”€â”€ ğŸ“œ Dockerfile                   # ECS å®¹å™¨é•œåƒçš„ Dockerfile
â”‚   â”œâ”€â”€ ğŸ“œ unzip.py                     # è§£å‹é€»è¾‘è„šæœ¬
â”‚   â”œâ”€â”€ ğŸ“œ requirements.txt             # ECS ä¾èµ–åŒ…
â”‚   â”œâ”€â”€ ğŸ“œ entrypoint.sh                # ECS ä»»åŠ¡å¯åŠ¨è„šæœ¬
â”œâ”€â”€ ğŸ“‚ terraform
â”‚   â”œâ”€â”€ ğŸ“œ main.tf                      # Terraform ä¸»é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ ğŸ“œ variables.tf                 # å˜é‡å®šä¹‰
â”‚   â”œâ”€â”€ ğŸ“œ outputs.tf                   # è¾“å‡ºé…ç½®
â”‚   â”œâ”€â”€ ğŸ“œ provider.tf                  # AWS Provider é…ç½®
â”‚   â”œâ”€â”€ ğŸ“œ iam.tf                       # IAM è§’è‰²ä¸æƒé™é…ç½®
â”‚   â”œâ”€â”€ ğŸ“œ batch.tf                     # AWS Batch ä»»åŠ¡é…ç½®
â”‚   â”œâ”€â”€ ğŸ“œ s3.tf                        # S3 èµ„æºåˆ›å»º
```

---

## **ğŸ“œ `terraform/main.tf`** (Terraform ä¸»é…ç½®æ–‡ä»¶)

```hcl
provider "aws" {
  region = "us-east-1"
}

module "s3" {
  source = "./s3.tf"
}

module "iam" {
  source = "./iam.tf"
}

module "batch" {
  source     = "./batch.tf"
  ecs_role   = module.iam.ecs_task_role
  s3_bucket  = module.s3.s3_output_bucket
}
```

---

## **ğŸ“œ `terraform/s3.tf`** (S3 èµ„æº)

```hcl
resource "aws_s3_bucket" "s3input" {
  bucket = "my-batch-input-bucket"
}

resource "aws_s3_bucket" "s3output" {
  bucket = "my-batch-output-bucket"
}
```

---

## **ğŸ“œ `terraform/iam.tf`** (IAM è§’è‰²ä¸æƒé™)

```hcl
resource "aws_iam_role" "ecs_task_role" {
  name = "ecsTaskRole"
  assume_role_policy = <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "ecs-tasks.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF
}
```

---

## **ğŸ“œ `terraform/batch.tf`** (AWS Batch ä»»åŠ¡é…ç½®)

```hcl
resource "aws_batch_job_definition" "unzip_job" {
  name = "unzip-job"
  type = "container"

  container_properties = jsonencode({
    image = "ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com/unzip-processor:latest"
    vcpus = 1
    memory = 512
    jobRoleArn = aws_iam_role.ecs_task_role.arn
    environment = [
      { name = "S3_INPUT_BUCKET", value = "my-batch-input-bucket" },
      { name = "S3_OUTPUT_BUCKET", value = "my-batch-output-bucket" },
      { name = "FILES_TO_PROCESS", value = "" }  # From Lambda dynamically injected
    ]
  })
}
```

---

## **ğŸ“œ `ecs-unzip/Dockerfile`** (ECS å®¹å™¨é•œåƒ Dockerfile)

```dockerfile
FROM python:3.9

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY unzip.py .
COPY entrypoint.sh .

ENTRYPOINT ["/bin/bash", "entrypoint.sh"]
```

---

## **ğŸ“œ `ecs-unzip/requirements.txt`** (ECS ä¾èµ–)

```
boto3
```

---

## **ğŸ“œ `ecs-unzip/unzip.py`** (è§£å‹é€»è¾‘ Python è„šæœ¬)

```python
import os
import tarfile
import boto3

s3 = boto3.client("s3")
input_bucket = os.environ["S3_INPUT_BUCKET"]
output_bucket = os.environ["S3_OUTPUT_BUCKET"]

# ä»ç¯å¢ƒå˜é‡è·å–è¦å¤„ç†çš„æ–‡ä»¶
files_to_process = os.environ["FILES_TO_PROCESS"].split(",")

def extract_files():
    for file_key in files_to_process:
        # ä¸‹è½½æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        local_file = f"/tmp/{file_key.split('/')[-1]}"
        s3.download_file(input_bucket, file_key, local_file)

        # è§£å‹æ–‡ä»¶
        with tarfile.open(local_file, "r:gz") as tar:
            tar.extractall("/tmp/extracted")

        # ä¸Šä¼ è§£å‹åçš„æ–‡ä»¶åˆ° S3
        for file_name in os.listdir("/tmp/extracted"):
            s3.upload_file(f"/tmp/extracted/{file_name}", output_bucket, file_name)

def main():
    extract_files()

if __name__ == "__main__":
    main()
```

---

## **ğŸ“œ `ecs-unzip/entrypoint.sh`** (ECS ä»»åŠ¡å¯åŠ¨è„šæœ¬)

```bash
#!/bin/bash

# æ‰§è¡Œè§£å‹ä»»åŠ¡
python3 /app/unzip.py
```

---

## **ğŸ“œ `lambda/lambda_function.py`** (è§¦å‘ AWS Batch ä»»åŠ¡çš„ Lambda å‡½æ•°)

```python
import boto3
import json
import math

# S3 å’Œ AWS Batch å®¢æˆ·ç«¯
s3 = boto3.client("s3")
batch = boto3.client("batch")

S3_BUCKET = "my-batch-input-bucket"
BATCH_JOB_QUEUE = "unzip-queue"
BATCH_JOB_DEFINITION = "unzip-job"

def lambda_handler(event, context):
    # è·å– S3 ä¸­æ‰€æœ‰ tar.gz æ–‡ä»¶
    response = s3.list_objects_v2(Bucket=S3_BUCKET)
    files = [item["Key"] for item in response.get("Contents", []) if item["Key"].endswith(".tar.gz")]

    # è®¡ç®—éœ€è¦å¤šå°‘ä¸ªä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡æœ€å¤šå¤„ç†2ä¸ªæ–‡ä»¶
    num_files = len(files)
    num_tasks = math.ceil(num_files / 2)

    # æäº¤å¤šä¸ª AWS Batch ä»»åŠ¡
    job_ids = []
    for i in range(num_tasks):
        # æ¯ä¸ªä»»åŠ¡å°†å¤„ç†ä¸¤ä¸ªæ–‡ä»¶ï¼Œç¡®ä¿æœ€å¤šå¤„ç† 2 ä¸ªæ–‡ä»¶
        task_files = files[i * 2: (i + 1) * 2]
        
        # æäº¤ Batch ä»»åŠ¡
        response = batch.submit_job(
            jobName=f"unzip-job-{i}",
            jobQueue=BATCH_JOB_QUEUE,
            jobDefinition=BATCH_JOB_DEFINITION,
            containerOverrides={
                "environment": [
                    {"name": "FILES_TO_PROCESS", "value": ",".join(task_files)},
                    {"name": "S3_INPUT_BUCKET", "value": S3_BUCKET},
                    {"name": "S3_OUTPUT_BUCKET", "value": "my-batch-output-bucket"}
                ]
            }
        )
        job_ids.append(response["jobId"])

    return {
        "statusCode": 200,
        "body": json.dumps({
            "message": "Batch jobs submitted successfully.",
            "job_ids": job_ids
        })
    }
```

---

## **ğŸ“œ `lambda/calculate_time_lambda.py`** (è®¡ç®—è§£å‹æ€»æ—¶é—´çš„ Lambda å‡½æ•°)

```python
import boto3
import json
import time

s3 = boto3.client("s3")
S3_BUCKET = "my-batch-output-bucket"

def lambda_handler(event, context):
    # è·å–å¼€å§‹æ—¶é—´
    start_obj = s3.get_object(Bucket=S3_BUCKET, Key="unzip_status/start_time.json")
    start_time = json.loads(start_obj["Body"].read().decode("utf-8"))["start_time"]

    # è·å–æ‰€æœ‰ä»»åŠ¡ç»“æŸæ—¶é—´
    response = s3.list_objects_v2(Bucket=S3_BUCKET, Prefix="unzip_status/")
    end_times = []
    
    for item in response.get("Contents", []):
        if "start_time.json" not in item["Key"]:
            obj = s3.get_object(Bucket=S3_BUCKET, Key=item["Key"])
            job_data = json.loads(obj["Body"].read().decode("utf-8"))
            end_times.append(job_data["end_time"])

    # è®¡ç®—æœ€é•¿ç»“æŸæ—¶é—´
    if end_times:
        total_time = max(end_times) - start_time
    else:
        total_time = None

    # ç”Ÿæˆ CSV ç»“æœ
    csv_content = f"Start_Time,End_Time,Total_Time\n{start_time},{max(end_times)},{total_time}\n"
    s3.put_object(Bucket=S3_BUCKET, Key="unzip_results.csv", Body=csv_content)

    return {"start_time": start_time, "end_time": max(end_times), "total_time": total_time}
```

---

## **ğŸ“œ `lambda/zip_lambda.sh`** (æ‰“åŒ… Lambda ä»£ç è„šæœ¬)

```sh
cd lambda
zip -r lambda_function.zip lambda_function
